# AstroAirk - Project Structure

## ğŸ“ Directory Organization

```
astroairk/
â”œâ”€â”€ main.py                      # Application entry point (Gradio UI)
â”œâ”€â”€ config.py                    # Configuration management
â”œâ”€â”€ niche_config.py              # Niche-specific configurations
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env                         # Environment variables (not in git)
â”œâ”€â”€ .env.example                 # Example environment configuration
â”‚
â”œâ”€â”€ agents/                      # Core AI agents
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ openrouter_synthesizer.py    # GPT-4.1 Mini final synthesis (PRIMARY)
â”‚   â”œâ”€â”€ modern_synthesizer.py        # Gemini synthesis (fallback)
â”‚   â”œâ”€â”€ smart_orchestrator.py        # Main orchestration logic
â”‚   â”œâ”€â”€ simple_chart_parser.py       # Chart data parser
â”‚   â”œâ”€â”€ question_complexity.py       # Question classifier
â”‚   â”œâ”€â”€ semantic_selector.py         # Semantic factor selection
â”‚   â”œâ”€â”€ cached_retriever.py          # Parallel RAG retrieval with caching
â”‚   â”œâ”€â”€ real_rag_retriever.py        # Vertex AI RAG integration
â”‚   â”œâ”€â”€ vector_search_retriever.py   # Vector search fallback
â”‚   â”œâ”€â”€ niche_preloader.py           # Knowledge pre-loading system
â”‚   â”œâ”€â”€ fast_reranker.py             # NumPy-based reranking (53x faster)
â”‚   â”œâ”€â”€ gemini_embeddings.py         # Embedding generation
â”‚   â””â”€â”€ validator.py                 # Response validation
â”‚
â”œâ”€â”€ utils/                       # Utility modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cache_manager.py             # Two-level caching system (Redis + in-memory)
â”‚   â””â”€â”€ conversation_manager.py      # Conversation session management
â”‚
â”œâ”€â”€ niche_instructions/          # Niche-specific prompts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ love.py                      # Love & Relationships
â”‚   â”œâ”€â”€ career.py                    # Career & Profession
â”‚   â”œâ”€â”€ health.py                    # Health & Wellness
â”‚   â”œâ”€â”€ wealth.py                    # Finance & Wealth
â”‚   â””â”€â”€ spiritual.py                 # Spiritual Growth
â”‚
â””â”€â”€ docs/                        # Documentation
    â”œâ”€â”€ README.md                    # Project overview
    â””â”€â”€ QUICK_START.md               # Quick start guide
```

## ğŸ§© Core Components

### **1. Main Application** (`main.py`)
- Gradio web interface
- Session management
- Request routing
- Performance tracking

### **2. Synthesis Engine**
- **Primary:** OpenRouter GPT-4.1 Mini (1-2s draft, 3-5s detailed)
- **Fallback:** Gemini 2.5 Flash
- **Modes:** Draft (500 tokens) vs Expand (1500 tokens)

### **3. RAG Pipeline**
```
User Question
    â†“
Question Classifier (SIMPLE/MODERATE/COMPLEX)
    â†“
Semantic Factor Selector (chart highlights)
    â†“
Knowledge Pre-loader (351 passages cached)
    â†“
Parallel RAG Retrieval (6 passages)
    â†“
Fast NumPy Reranker (â†’ top 3)
    â†“
Final Synthesis (GPT-4.1 Mini)
    â†“
Response (with citations)
```

### **4. Caching System**
- **Level 1:** Intent + Chart bucket (hot intents, 24h TTL)
- **Level 2:** Full prompt cache (6h TTL)
- **Backend:** Redis (primary) + in-memory LRU (fallback)

## ğŸš€ Performance Characteristics

| Metric | Value |
|--------|-------|
| **Draft Response** | 1-2 seconds |
| **Detailed Response** | 3-5 seconds |
| **Cache Hit Rate** | 35-45% |
| **RAG Retrieval** | 650-900ms (merged query) |
| **Reranking** | 15ms (NumPy fast) |
| **Pre-load Time** | 37s (351 passages, 117 factors) |

## ğŸ“¦ Dependencies

### Core Libraries
- `gradio` - Web UI framework
- `google-genai` - Vertex AI integration
- `requests` - HTTP client (for OpenRouter)
- `numpy` - Fast numerical operations
- `python-dotenv` - Environment management

### Google Cloud
- Vertex AI RAG API
- Gemini embeddings (text-embedding-004)
- Gemini models (fallback synthesis)

### Optional
- `redis` - Production caching (falls back to in-memory)

## ğŸ”§ Configuration

### Environment Variables (`.env`)
```bash
# Google Cloud
GCP_PROJECT_ID=your-project-id
GCP_REGION=asia-south1
RAG_CORPUS_ID=your-corpus-id
GOOGLE_CLOUD_API_KEY=your-api-key

# OpenRouter (Primary Synthesis)
OPENROUTER_API_KEY=sk-or-v1-...

# Model Settings
MODEL_NAME=gemini-1.5-flash
TEMPERATURE=0.2
MAX_OUTPUT_TOKENS=2400

# RAG Settings
USE_REAL_RAG=true
RAG_TOP_K=6
RAG_SIMILARITY_THRESHOLD=0.5

# Server
PORT=8080
DEBUG=false
```

## ğŸ¯ Key Features

### âœ… Implemented
1. **Two-pass generation** (Draft/Expand modes)
2. **Merged RAG queries** (4â†’1 call, 3.2x faster)
3. **Fast NumPy reranker** (53x faster than LLM)
4. **Two-level caching** (Redis + in-memory)
5. **Knowledge pre-loading** (351 passages cached)
6. **Semantic factor selection** (chart highlights)
7. **Comprehensive timing instrumentation**
8. **Session-based conversations**
9. **OpenRouter GPT-4.1 Mini integration** (primary synthesis)

### ğŸš§ Future Enhancements
1. Long-term conversational memory (5-message summarization)
2. Cross-session memory persistence
3. User profile memory
4. Answer cache with cross-user sharing
5. Passage cache pre-warming
6. Background summarization

## ğŸ“Š Code Quality

### Standards
- Type hints throughout
- Comprehensive logging
- Error handling with fallbacks
- Response caching
- Token budget management

### Testing
```bash
# Compile check
python -m compileall .

# RAG test
python test_rag.py

# Full system test
USE_REAL_RAG=true python main.py
```

## ğŸ”’ Security

- API keys in `.env` (not in git)
- `.gitignore` for sensitive files
- Encrypted transit (TLS)
- Rate limiting (TODO)
- Input validation

## ğŸ“ Development Workflow

1. **Setup:**
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -r requirements.txt
   cp .env.example .env  # Edit with your keys
   ```

2. **Run:**
   ```bash
   USE_REAL_RAG=true python main.py
   ```

3. **Deploy:**
   ```bash
   gcloud builds submit --config cloudbuild.yaml
   ```

## ğŸ’¡ Architecture Highlights

### Why OpenRouter + GPT-4.1 Mini?
- **No thinking overhead** (unlike Gemini 2.5 Flash)
- **3x faster** token-to-token generation
- **46% cheaper** per token
- **Same quality** for Vedic astrology synthesis

### Why Two-Level Caching?
- **Level 1:** Reuses answers for similar intents (35% hit rate)
- **Level 2:** Reuses RAG passages (saves 700ms on hits)
- **Combined:** 30-40% average speedup

### Why Fast Reranker?
- **NumPy vectorization:** 53x faster than LLM reranking
- **15ms latency:** vs 800ms for LLM
- **Same quality:** Cosine + IDF + tag matching

## ğŸ“ Learning Resources

- [Vertex AI RAG](https://cloud.google.com/vertex-ai/docs/generative-ai/rag-overview)
- [OpenRouter API](https://openrouter.ai/docs)
- [Gradio Documentation](https://www.gradio.app/docs)

## ğŸ“ Support

For issues or questions, check:
1. `README.md` - Project overview
2. `QUICK_START.md` - Quick start guide
3. This file - Project structure
